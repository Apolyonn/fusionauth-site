---
layout: doc
title: Migration guide
description: How to migrate your users to FusionAuth
---
:page-liquid:

== Overview

This document will help you migrate your existing users to FusionAuth. 

== Types of Migrations

There are three types of migrations. 

* Migrate everyone at once, also known as a "big bang" migration.
* Segment your users and migrate each segment in a series of small bang migrations.
* Migrate on authentication, also known as "slow migration".

Each of these moves user and other account associated data into FusionAuth from one or more other systems of record. These each have strengths and weaknesses. All three are supported by FusionAuth, so you should pick the one which works best for your situation.

=== The Big Bang Migration

With a big bang migration, you are moving all your users at one moment in time. The basic steps are:

* Map user attributes from the old system to the new system.
* Build a set of migration scripts or programs. 
* Test it well; check for migration accuracy as well as duration. 
* Plan to modify your applications to point to the new system.
* When you are ready to migrate, arrange for an adequate period of downtime, then run the scripts or programs.
* Flip the system of record for all your users from the old to the new.

This approach has some strengths:

* Depending on the downtime, the migration can have minimal impact on your users.
* It has a fixed timeframe. When you are done with the migration, you're done (unless you have to rollback).
* If you have to decomission the old system by a deadline (perhaps due to an upcoming license renewal) you can plan to migrate before that date.
* You only have to run two user datastores in production for a short period of time.
* People accessing user data, such as customer service reps, only need to switch their working routines after the migration is performed.

It also has some downsides:

* It is common to miss an issue during testing because by definition this is a one time process. Production systems are often different in subtle ways from testing environments. Leave time to fix unpleasant surprises.
* If there is any issue, many users will be impacted, since all were migrated.
* The big bang requires you to write code which you'll test intensely, use once and then throw away. Such code is the cost for the benefits.
* You need to ensure the new auth system is compatible with the old system passwords hashes. The alternative is to force all users to reset their password.

=== Segment by Segment Migration

Segment by segment migration is the second alternative. It can be thought of as a series of "little bang" migrations. Split user data in a natural way, then migrate each segment. Some natural division points could be type of user, applications used or even recency of login.

Such a segment by segment migration lets you test your processes in production by migrating less critical, or more understanding, sets of users first. The engineering team will be more understanding of issues than paying customers. 

You will probably be able to reuse code between the different segments. In general, this approach decreases risk when compared to the one time big bang migration.

However, this approach is not without its issues:

* You have multiple projects and downtimes to manage, not just one.
* There may be no natural divisions in your user base.
* If most of the users are in one segment, it may not be worth the extra effort. For example, if you have one popular application and a couple nascent apps, the extra time to migrate in phases might not be worth it.
* This will take longer to complete, requiring you to run both systems for longer.

=== Migrate On Authentication

This approach is the logical extension of the segment by segment approach; each segment is a single user. However, it has different fundamental characteristics and presents a different set of tradeoffs.

With a slow migration, you set up a connection between the old system and the new system. The new system takes all auth requests, but delegates them to the old system. The old system returns the information and the new system stores it.

To implement this, you need to have some way for the new system to pass auth information (username and password or other credentials) to the old system and get user information returned. And you need to modify your applications to point to the new auth system.

Once it is set up, a slow migration happens in phases. Each user proceeds through the phases independently of other users. 

* Initially, the user logs into the new system, but authentication decisions are delegated to the old system.
* The old system returns the user information, which is stored in the new system. This includes the password, which is hashed by the new system.
* The new system marks the user as migrated.
* The new system performs authentication for this user for all subsequent logins.

Migrate on authentication has some benefits:

* Since you are only doing a user migration at the time a user authenticates, by definition the blast radius of a mistake is small; it's limited to whoever is logging in. You can decrease this further by splitting traffic and only sending a portion to the new system.
* You can upgrade your password hash algorithms transparently without requiring anyone to reset their password.
* You don't have to migrate users who aren't active. This migration is a way to scrub your userbase.
* The migration can be an opportunity to contact inactive users and encourage them to login.
* There's less downtime; you aren't moving any data. Depending on your system architecture, there may be some downtime as you direct all your users to the new auth system, rather than the old one.
* You don't have to understand all the moving pieces of the old system. You do need to be able to extend it to add the API, but you never have to connect to the underlying user datastore or understand all the business logic which goes into authentication.

However, a slow migration isn't the right solution for every system. Issues to be aware of:

* You are passing a user's plaintext password from the new auth system to the old auth system. Therefore, you should take special care to secure this data in transit. For an additional layer of security, have the new auth system encrypt the password, and have the old auth system endpoint decrypt it before authenticating.
* Both the new and old systems have to be extensible or support a standard like LDAP.
* You have to run both the new and old systems for the length of your migration. Depending on the state of the old system, this may be more or less painful.
* Customer service and other internal users may have to access two systems to find a user during the migration period.
* Rollback from a phased migration is more complex if there are any issues, because there are two systems of record, one for users who have been migrated and one for users who have not yet been moved.

== Migration Implementation

To implement any of the above migration strategies and bring your users into FusionAuth, there are N phases.

=== Planning and Assessment

The first step to any successful data migration is planning. You need to know where all your data sources are, who uses the data from the sources, how to connect to them, and what the data looks like.

Consider edge cases. What fields are required and optional in the old system (or systems)? What about the new system? Is there a clean mapping between the data fields? The answer is almost certainly no. 

Think about how you are going to handle data you don't expect. You can save the entire record off for manual migration, toss them, or ignore fields that aren't a clean match.

With FusionAuth, you have the option of storing arbitrary key value data. , serialize the user object from the old system and store it there. The old user model may be helpful in the future because if there were data that mistranslated, you’ll have the original copy.

You’ll also want to think about relationships between users and other objects. Groups, roles, historical data, and anything else the old system has tied to users. Make sure you know where this data is coming from, if it can be discarded, and if not, where it will end up. Perhaps it will be migrated to the new system, perhaps to a different datastore.

There are two field types worth commenting on in more detail. The first is user ids. These are often referenced by other systems and are used for auditing, analytics or other historical purposes. If you can preserve user ids, do so.

If you cannot, plan accordingly. You may want to keep that field in the new system in an old_user_id field, accept the loss of this data, or build a system to map from old user ids to new user ids, to be used by any external party which depended on the old user id.

The other notable attribute is the password, and any related fields such as a salt. Passwords don’t have to be migrated in a slow migration. The user will provide the password during authentication, and it can be stored in the new auth system during that process.

One special complexity which will require more planning is if you have user data in multiple datastores and are planning to merge the data. Ensure you map both of the old data models into the new user data model before you write any migration code.

==== An Example 

Let's examine a trivial data mapping example. Suppose an old auth system has a user data model with these attribute names and data types:

* `fname` - string
* `lname` - string
* `birthdate` - string
* `phone_num` - string
* `role` - string

FusionAuth has a user object with these attributes and data types:

* `firstName` - string
* `lastName` - string
* `birthDate` - An ISO-8601 formatted date string
* `mobilePhone` - string

When you are moving between them, you’ll face three challenges.

The first mapping is converting from `fname` to `first_name` and `lname` to `last_name`. This is pretty easy. Where this gets difficult is that you need to make sure you do this for each and every one of the old data sources. Spreadsheets are your friend.

The second mapping task is parsing the `birthdate` field into an ISO-8601 formatted string, to be placed in the `birthDate` FusionAuth field. Depending on how clean the original data is, this could be trivial or it could be painful. If the latter, use a date parsing library; it'll handle many more edge cases than your code.

The third issue is how to handle the `phone_num` field. In this case, if you
data field

Finally, we nee dtRole

Here's a sample FusionAuth user object which should give you an idea of what fields are available:

[source,json]
.Example User JSON
----
include::../../../src/json/user/request.json[]
----

Please consult the link:docs/tech/v1/apis/users/[User API] for full documentation. Here's a sample FusionAuth registration object.

[source,json]
.Example User Registration JSON
----
include::../../../src/json/user-registrations/request.json[]
----

Please consult the link:docs/tech/v1/apis/users/[Registration API] for full documentation.


As this example shows, getting ready for a migration consists of many tiny choices and design decisions. Make sure you understand the data model for your users before you start the migration.



=== Setting up FusionAuth

=== Big Bang Implementation

=== Migrate on Authentication Implementation

How to implement

common implementation details

from the migration tutorial

big bang

password encryptors
import users
import refresh tokens


Migration guide

Tenant stuff

Switch to database search engine if possible

Depends on how you build the import request. If you only provide a password field, then we will hash it for you. If you provide us the salt, password, encryptionScheme and factor then we will assume this is a hashed password, and it will not be hashed.
https://fusionauth.slack.com/archives/GNURTTC8N/p1602032895434200 


Ok. I would not recommend enabling the user.bulk.create webhook during import testing. If you do a 100k import - that will try to send you an event w/ 100k users.
If FusionAuth sent the 504 that would indicate it is due to a webhook TX failure. If you have your webhook configured to not require a success - then it is possible it is coming from somewhere else, a proxy for example due to a timeout. (edited) 
6:24
.. That is unless, you want to receive the user.bulk.create event and plan to utilize it, but if not, it is adding un-necessary load to the import request.
6:25
You will want to ensure you are setting your timeout very long on your API requests in whatever tool you’re using to make the HTTP request.
 
https://fusionauth.slack.com/archives/GNURTTC8N/p1601943864286800


Timeout for client/sync calls


The easiest and fastest way to load data into FusionAuth is to loop over a directory of JSON files that contain 100k users each. These should be clean and have no collisions and be minified. You can technically do this using 2 threads so that each thread is hitting different EC2 instances, but the database will always be the bottleneck.
https://inversoft.slack.com/archives/C051S8N8E/p1602077833045300 




test
downtime
switch

segments


slow migration

connectors

secure your connection

4 phases

determine finish point

determine what to do at end


additional resources
http://localhost:4000/docs/v1/tech/tutorials/migrate-users/
https://github.com/FusionAuth/fusionauth-import-scripts
